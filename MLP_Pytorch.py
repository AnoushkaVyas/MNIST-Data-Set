# -*- coding: utf-8 -*-
"""q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FcY0eYuqifQI7zziq78qEQxMTGKB5Rp1
"""

import torch
import torch.nn as nn
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import torch.nn.init as weight_init
import matplotlib.pyplot as plt
import pdb
from torch.optim.lr_scheduler import StepLR

preprocess = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])
train_dataset = datasets.MNIST(root='./data',  train=True, transform=preprocess,  download=True)
test_dataset = datasets.MNIST(root='./data',  train=False,  transform=preprocess)

print(len(train_dataset))
print(len(test_dataset))
print(train_dataset[0][0].size())

# Parameters 
input_size = 784
hidden_size = 256
num_classes = 10
num_epochs = 15
momentum_rate = 0.9
learning_rate=0.1
weight='x'

class Net(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes,weight):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size) 
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)
        self.softmax = nn.Softmax(dim=1)
        
        #Weight Initialization
        if weight=='x':
          for m in self.modules():
            if isinstance(m,nn.Linear):
              weight_init.xavier_normal_(m.weight)
        if weight=='u':
          for m in self.modules():
            if isinstance(m,nn.Linear):
              weight_init.uniform(m.weight,a=0,b=1)
        if weight == 'g':
          for m in self.modules():
            if isinstance(m,nn.Linear):
              weight_init.normal_(m.weight,mean=0.,std=1.)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.softmax(out)
        return out
net = Net(input_size, hidden_size, num_classes,weight)

#loss
def loss_plot(losses):
   max_epochs = len(losses)
   times = list(range(1, max_epochs+1))
   plt.figure(figsize=(30, 7)) 
   plt.title('Loss Curve for lr=0.1 and Gaussian initialisaiton') 
   plt.xlabel("epochs")
   plt.ylabel("cross-entropy loss")
   return plt.plot(times, losses)

# Dataloader
batch_size = 100

#loading the train dataset
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)

#loading the test dataset
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False)

for (X_train, y_train) in train_loader:
    print('X_train:', X_train.size(), 'type:', X_train.type())
    print('y_train:', y_train.size(), 'type:', y_train.type())
    break

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum_rate)
scheduler = StepLR(optimizer, step_size=1, gamma=0.1)

use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
print('Using CUDA ', use_cuda)
net = net.to(device)
# Training
epochLoss = []
for epoch in range(num_epochs):
    
    total_loss = 0
    cntr = 1
    scheduler.step()
    # For each batch of images in train set
    for i, (images, labels) in enumerate(train_loader):
        
        images = images.view(-1, 28*28)
        labels = labels
        
        images, labels = images.to(device), labels.to(device)
        
        # Initialize gradients to 0
        optimizer.zero_grad()
        
        # Forward pass (this calls the "forward" function within Net)
        outputs = net(images)
        
        # Find the loss
        loss = criterion(outputs, labels)
        
        # Backward pass (Find the gradients of all weights using the loss)
        loss.backward()
        
        # Update the weights using the optimizer
        # For e.g.: w = w - (delta_w)*lr
        optimizer.step()
        
        total_loss += loss.item()
        cntr += 1
    
    print('Epoch [%d/%d] Loss:%.4f'%(epoch+1, num_epochs, total_loss/cntr) )
    epochLoss.append(total_loss/cntr)

_ = loss_plot(epochLoss)

use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
print('Using CUDA ', use_cuda)
net = net.to(device)
correct = 0
total = 0

# For each batch of images in test set
with torch.no_grad():
  for images, labels in test_loader:

      # Get the images
      images = images.view(-1, 28*28)

      images = images.to(device)

      # Find the output by doing a forward pass through the network
      outputs = net(images)

      # Find the class of each sample by taking a max across the probabilities of each class
      _, predicted = torch.max(outputs.data, 1)

      # Increment 'total', and 'correct' according to whether the prediction was correct or not
      total += labels.size(0)
      correct += (predicted.cpu() == labels).sum()

print('Accuracy of the network on the 10000 test images for lr =0.1 and weight initialisation as gausiian: %d %%' % (100 * correct / total))